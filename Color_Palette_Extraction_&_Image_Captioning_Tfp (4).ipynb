{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQG19KlpzM-E"
      },
      "source": [
        "# By: Aya Joharji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oP5vUjhX150"
      },
      "source": [
        "# Color Palette Extraction & Image Captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Overview\n",
        "This project aims to create a tool that extracts a color palette from an uploaded image and generates a bilingual caption (in both English and Arabic) that describes the image. The tool leverages Hugging Face pipelines for image captioning and translation, combined with a user-friendly interface built using Gradio.\n",
        "\n",
        "**Motivation**\n",
        "The idea behind this project is to simplify the process of analyzing and describing images. It combines the visual aspect of color extraction, which can assist designers and artists in choosing color schemes, with descriptive captions that enhance image understanding. The inclusion of Arabic language support helps ensure accessibility for a broader audience, particularly Arabic speakers."
      ],
      "metadata": {
        "id": "IBKTqqeEvi9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Dependencies\n",
        "!pip install gradio transformers Pillow scikit-learn sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7ycPyKsdwMp3",
        "outputId": "e27d8084-5aa5-4b40-a5d7-7d99d701ac7d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from sklearn.cluster import KMeans\n",
        "from transformers import pipeline\n",
        "from PIL import Image, ImageDraw\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "QiXQrHRhwTBs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pipelines globally to avoid reloading on each inference\n",
        "print(\"Loading pipelines...\")\n",
        "\n",
        "# Image Captioning Pipeline\n",
        "# Using Salesforce/blip-image-captioning-base for generating image captions\n",
        "caption_pipeline = pipeline(\n",
        "    \"image-to-text\",\n",
        "    model=\"Salesforce/blip-image-captioning-base\"\n",
        ")\n",
        "\n",
        "# Translation Pipeline\n",
        "# Using facebook/mbart-large-50-many-to-many-mmt for translations\n",
        "# This model supports multiple languages and provides better translation quality for Arabic\n",
        "translation_pipeline = pipeline(\n",
        "    \"translation\",\n",
        "    model=\"facebook/mbart-large-50-many-to-many-mmt\",\n",
        "    tokenizer=\"facebook/mbart-large-50-many-to-many-mmt\",\n",
        "    src_lang=\"en_XX\",\n",
        "    tgt_lang=\"ar_AR\"\n",
        ")\n",
        "\n",
        "print(\"Pipelines loaded successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxKCzQJ8wc0x",
        "outputId": "58bc67da-4c70-47d5-9e0f-7c330ea95631"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pipelines...\n",
            "Pipelines loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Download Example Images\n",
        "def download_example_images():\n",
        "    \"\"\"\n",
        "    Downloads example images from provided URLs and saves them locally.\n",
        "\n",
        "    Returns:\n",
        "        examples (list): A list of file paths to the downloaded example images.\n",
        "    \"\"\"\n",
        "    # List of image descriptions and URLs\n",
        "    image_urls = [\n",
        "        # URL format: (\"Image Description\", \"Image URL\")\n",
        "        (\"Sunset over Mountains\",\"https://images.unsplash.com/photo-1501785888041-af3ef285b470?w=512\"),\n",
        "        (\"Forest Path\",\"https://images.unsplash.com/photo-1502082553048-f009c37129b9?w=512\"),\n",
        "        (\"City Skyline\",\"https://images.unsplash.com/photo-1498598453737-8913e843c47b?w=512\"),\n",
        "        (\"Beach and Ocean\",\"https://images.unsplash.com/photo-1507525428034-b723cf961d3e?w=512\"),\n",
        "        (\"Desert Dunes\",\"https://images.unsplash.com/photo-1501594907352-04cda38ebc29?w=512\"),\n",
        "        (\"Snowy Mountain Peak\",\"https://images.unsplash.com/photo-1519608487953-e999c86e7455?w=512\"),\n",
        "        (\"Autumn Leaves\",\"https://images.unsplash.com/photo-1500530855697-b586d89ba3ee?w=512\"),\n",
        "        (\"City Street at Night\",\"https://images.unsplash.com/photo-1512453979798-5ea266f8880c?w=512\"),\n",
        "        (\"Calm Lake Reflection\",\"https://images.unsplash.com/photo-1506744038136-46273834b3fb?w=512\"),\n",
        "        (\"Lush Green Hills\",\"https://images.unsplash.com/photo-1501696461280-37c52f57e8c1?w=512\"),\n",
        "    ]\n",
        "\n",
        "    examples = []\n",
        "    for idx, (description, url) in enumerate(image_urls, start=1):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                # Open the image and save it locally\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                img.save(f'example{idx}.jpg')\n",
        "                examples.append([f'example{idx}.jpg'])\n",
        "            else:\n",
        "                print(f\"Failed to download image from {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Exception occurred while downloading image: {e}\")\n",
        "    return examples\n",
        "\n",
        "# Download example images and prepare examples list\n",
        "examples = download_example_images()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B2W4_AgwoAU",
        "outputId": "569719cc-fa24-40fd-fef8-b4b489836508"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download image from https://images.unsplash.com/photo-1498598453737-8913e843c47b?w=512\n",
            "Failed to download image from https://images.unsplash.com/photo-1501696461280-37c52f57e8c1?w=512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Load and Process Image\n",
        "def load_image(image):\n",
        "    \"\"\"\n",
        "    Converts the input image to a numpy array and resizes it.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image): The input image.\n",
        "\n",
        "    Returns:\n",
        "        resized_image_np (numpy.ndarray): The resized image as a numpy array.\n",
        "    \"\"\"\n",
        "    # Convert PIL image to numpy array (RGB)\n",
        "    image_np = np.array(image.convert('RGB'))\n",
        "\n",
        "    # Resize the image to (300, 300) for consistent processing\n",
        "    resized_image = image.resize((300, 300), resample=Image.LANCZOS)\n",
        "    resized_image_np = np.array(resized_image)\n",
        "\n",
        "    return resized_image_np"
      ],
      "metadata": {
        "id": "NdX9CtddwyYZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Extract Dominant Colors from the Image\n",
        "def extract_colors(image, k=8):\n",
        "    \"\"\"\n",
        "    Uses KMeans clustering to extract dominant colors from the image.\n",
        "\n",
        "    Args:\n",
        "        image (numpy.ndarray): The input image as a numpy array.\n",
        "        k (int): The number of clusters (colors) to extract.\n",
        "\n",
        "    Returns:\n",
        "        colors (numpy.ndarray): An array of the dominant colors.\n",
        "    \"\"\"\n",
        "    # Flatten the image to a 2D array of pixels\n",
        "    pixels = image.reshape(-1, 3)\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    pixels = pixels / 255.0\n",
        "    pixels = pixels.astype(np.float64)\n",
        "\n",
        "    # Apply KMeans clustering to find dominant colors\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=k,\n",
        "        random_state=0,\n",
        "        n_init=10,\n",
        "        max_iter=300\n",
        "    )\n",
        "    kmeans.fit(pixels)\n",
        "\n",
        "    # Convert normalized colors back to 0-255 scale\n",
        "    colors = (kmeans.cluster_centers_ * 255).astype(int)\n",
        "    return colors"
      ],
      "metadata": {
        "id": "RvqUb1ukw1tg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Create an Image for the Color Palette\n",
        "def create_palette_image(colors):\n",
        "    \"\"\"\n",
        "    Creates a visual representation of the color palette.\n",
        "\n",
        "    Args:\n",
        "        colors (numpy.ndarray): An array of the dominant colors.\n",
        "\n",
        "    Returns:\n",
        "        palette_image (PIL.Image.Image): The generated color palette image.\n",
        "    \"\"\"\n",
        "    num_colors = len(colors)\n",
        "    palette_height = 100\n",
        "    palette_width = 100 * num_colors\n",
        "    palette_image = Image.new(\n",
        "        \"RGB\",\n",
        "        (palette_width, palette_height)\n",
        "    )\n",
        "\n",
        "    draw = ImageDraw.Draw(palette_image)\n",
        "    for i, color in enumerate(colors):\n",
        "        # Ensure color values are within valid range and integers\n",
        "        color = tuple(np.clip(color, 0, 255).astype(int))\n",
        "        # Draw rectangles for each color\n",
        "        draw.rectangle(\n",
        "            [i * 100, 0, (i + 1) * 100, palette_height],\n",
        "            fill=color\n",
        "        )\n",
        "\n",
        "    return palette_image\n",
        "\n",
        "# Function to Display Color Palette as Hex Codes\n",
        "def display_palette(colors):\n",
        "    \"\"\"\n",
        "    Converts RGB colors to hexadecimal format.\n",
        "\n",
        "    Args:\n",
        "        colors (numpy.ndarray): An array of the dominant colors.\n",
        "\n",
        "    Returns:\n",
        "        hex_colors (list): A list of hex color codes.\n",
        "    \"\"\"\n",
        "    hex_colors = []\n",
        "    for color in colors:\n",
        "        # Ensure color values are within valid range and integers\n",
        "        color = np.clip(color, 0, 255).astype(int)\n",
        "        # Convert to hex code\n",
        "        hex_color = \"#{:02x}{:02x}{:02x}\".format(\n",
        "            color[0],\n",
        "            color[1],\n",
        "            color[2]\n",
        "        )\n",
        "        hex_colors.append(hex_color)\n",
        "\n",
        "    return hex_colors"
      ],
      "metadata": {
        "id": "AXxJvkRcw-CB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \"\"\"\n",
        "    convert (34, 139, 34) to a hex color:\n",
        "\n",
        "    Red (34):\n",
        "\n",
        "    Decimal 34 in hexadecimal is 22.\n",
        "    {:02x} will convert 34 to 22.\n",
        "    Green (139):\n",
        "\n",
        "    Decimal 139 in hexadecimal is 8b.\n",
        "    {:02x} will convert 139 to 8b.\n",
        "    Blue (34):\n",
        "\n",
        "    Decimal 34 in hexadecimal is 22.\n",
        "    {:02x} will convert 34 to 22.\n",
        "    Putting it all together:\n",
        "\n",
        "    The final hex color will be #228b22.\n",
        "\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "iHmunnYptd6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to Generate Image Caption Using Pipeline\n",
        "def generate_caption(image):\n",
        "    \"\"\"\n",
        "    Generates a caption for the input image using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image): The input image.\n",
        "\n",
        "    Returns:\n",
        "        caption (str): The generated caption.\n",
        "    \"\"\"\n",
        "    # Use the captioning pipeline to generate a caption\n",
        "    result = caption_pipeline(image)\n",
        "    caption = result[0]['generated_text']\n",
        "    return caption\n",
        "\n",
        "# Function to Translate Caption to Arabic Using Pipeline\n",
        "def translate_to_arabic(text):\n",
        "    \"\"\"\n",
        "    Translates English text to Arabic using a pre-trained model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The English text to translate.\n",
        "\n",
        "    Returns:\n",
        "        translated_text (str): The translated Arabic text.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the translation pipeline to translate the text\n",
        "        result = translation_pipeline(text)\n",
        "        translated_text = result[0]['translation_text']\n",
        "\n",
        "        # Post-processing to remove repeated words\n",
        "        words = translated_text.split()\n",
        "        seen = set()\n",
        "        cleaned_words = []\n",
        "        for word in words:\n",
        "            if word not in seen:\n",
        "                cleaned_words.append(word)\n",
        "                seen.add(word)\n",
        "        cleaned_translated_text = ' '.join(cleaned_words)\n",
        "\n",
        "        return cleaned_translated_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error during translation: {e}\")\n",
        "        return \"Translation Error\"\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z6-FyjA42ULk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio Interface Function (Combining All Elements)\n",
        "def process_image(image):\n",
        "    \"\"\"\n",
        "    Processes the input image to generate a bilingual caption and color palette.\n",
        "\n",
        "    Args:\n",
        "        image (PIL.Image.Image or numpy.ndarray): The input image.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Contains bilingual caption, hex color codes, palette image, and resized image.\n",
        "    \"\"\"\n",
        "    # Ensure input is a PIL Image\n",
        "    if isinstance(image, np.ndarray):\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    # Convert to RGB format\n",
        "    image_rgb = image.convert(\"RGB\")\n",
        "\n",
        "    # Load and resize the image\n",
        "    resized_image_np = load_image(image_rgb)\n",
        "    resized_image_pil = Image.fromarray(resized_image_np)\n",
        "\n",
        "    # Generate caption using the caption pipeline\n",
        "    caption = generate_caption(image_rgb)\n",
        "\n",
        "    # Translate caption to Arabic using the translation pipeline\n",
        "    caption_arabic = translate_to_arabic(caption)\n",
        "\n",
        "    # Extract dominant colors from the image\n",
        "    colors = extract_colors(resized_image_np, k=8)\n",
        "    color_palette = display_palette(colors)\n",
        "\n",
        "    # Create palette image\n",
        "    palette_image = create_palette_image(colors)\n",
        "\n",
        "    # Combine English and Arabic captions\n",
        "    bilingual_caption = f\"English: {caption}\\nArabic: {caption_arabic}\"\n",
        "\n",
        "    return (\n",
        "        bilingual_caption,\n",
        "        \", \".join(color_palette),\n",
        "        palette_image,\n",
        "        resized_image_pil\n",
        "    )"
      ],
      "metadata": {
        "id": "QHkr1lvX5wIy"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Gradio Interface using Blocks and add a submit button\n",
        "with gr.Blocks(\n",
        "    css=\".gradio-container { height: 1000px !important; }\"\n",
        ") as demo:\n",
        "    # Title and Description\n",
        "    gr.Markdown(\n",
        "        \"<h1 style='text-align: center;'>\"\n",
        "        \"Palette Generator from Image with Image Captioning\"\n",
        "        \"</h1>\"\n",
        "    )\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <p style='text-align: center;'>\n",
        "        Upload an image or select one of the example images below to generate\n",
        "        a color palette and a description of the image in both English and Arabic.\n",
        "        </p>\n",
        "        \"\"\"\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            # Image Input Component\n",
        "            image_input = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Upload your image or select an example below\"\n",
        "            )\n",
        "            # Submit Button\n",
        "            submit_button = gr.Button(\"Submit\")\n",
        "            # Examples Component\n",
        "            gr.Examples(\n",
        "                examples=examples,\n",
        "                inputs=image_input,\n",
        "                label=\"Example Images\",\n",
        "                examples_per_page=5,\n",
        "            )\n",
        "        with gr.Column(scale=1):\n",
        "            # Output Components\n",
        "            caption_output = gr.Textbox(\n",
        "                label=\"Bilingual Caption\",\n",
        "                lines=5,\n",
        "                max_lines=10\n",
        "            )\n",
        "            palette_hex_output = gr.Textbox(\n",
        "                label=\"Color Palette Hex Codes\",\n",
        "                lines=2\n",
        "            )\n",
        "            palette_image_output = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Color Palette\"\n",
        "            )\n",
        "            resized_image_output = gr.Image(\n",
        "                type=\"pil\",\n",
        "                label=\"Resized Image\"\n",
        "            )\n",
        "\n",
        "    # Define the action on submit button click\n",
        "    submit_button.click(\n",
        "        fn=process_image,\n",
        "        inputs=image_input,\n",
        "        outputs=[\n",
        "            caption_output,\n",
        "            palette_hex_output,\n",
        "            palette_image_output,\n",
        "            resized_image_output\n",
        "        ],\n",
        "    )\n",
        "\n",
        "# Launch Gradio Interface\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "cxmulqwNxOKi",
        "outputId": "907ccc00-7b17-435c-9556-4712f7914210"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://22ad8734c11335db5a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://22ad8734c11335db5a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e8LkVxTAtkpR"
      },
      "execution_count": 20,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}